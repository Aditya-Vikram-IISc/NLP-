{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simple CNN for Sentiment Analysis.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2_JjD6Irmhw"
      },
      "source": [
        "## 1. Importing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFvKpDMLn7vH"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import re\n",
        "import math\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXbRdoi4n7x3"
      },
      "source": [
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVYnb0CstKh4"
      },
      "source": [
        "## 2. Loading Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWVggutFn72k",
        "outputId": "1777e959-ec18-4f6d-e5a2-29d539d22e31"
      },
      "source": [
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpMexyn6n70Q"
      },
      "source": [
        "cols = [\"sentiment\" , \"id\", \"date\", \"query\", \"user\", \"text\"]\n",
        "\n",
        "train_data = pd.read_csv(\"/content/drive/MyDrive/training_twitter.csv\", header = None, \n",
        "                         names = cols, engine = \"python\",\n",
        "                         encoding = \"latin1\")\n",
        "\n",
        "\n",
        "test_data = pd.read_csv(\"/content/drive/MyDrive/testdata_twitter.csv\", header = None, \n",
        "                         names = cols, engine = \"python\",\n",
        "                         encoding = \"latin1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjXBT1Y4tdre",
        "outputId": "655fdc29-78a9-4f45-afe4-44171217fc59"
      },
      "source": [
        "train_data[\"sentiment\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4    800000\n",
              "0    800000\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahVM0eIltdvM",
        "outputId": "945151fb-3609-4470-e007-f35e1ae08ea9"
      },
      "source": [
        "test_data[\"sentiment\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4    182\n",
              "0    177\n",
              "2    139\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "F9VJtMA4tdzZ",
        "outputId": "87f10d2c-2f0f-4a67-fd11-a2444d24eaa0"
      },
      "source": [
        "train_data.head(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>id</th>\n",
              "      <th>date</th>\n",
              "      <th>query</th>\n",
              "      <th>user</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810369</td>\n",
              "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>_TheSpecialOne_</td>\n",
              "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810672</td>\n",
              "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>scotthamilton</td>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810917</td>\n",
              "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>mattycus</td>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentiment  ...                                               text\n",
              "0          0  ...  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
              "1          0  ...  is upset that he can't update his Facebook by ...\n",
              "2          0  ...  @Kenichan I dived many times for the ball. Man...\n",
              "\n",
              "[3 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuuVWtCo6EyE",
        "outputId": "8e56558b-6c92-4cb8-c53d-c3a12b46aa25"
      },
      "source": [
        "print(\"The length of train_dataset is: \" + str(len(train_data)))\n",
        "print(\"The length of test_dataset is: \" + str(len(test_data)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The length of train_dataset is: 1600000\n",
            "The length of test_dataset is: 498\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0OXkQZA0xdZ"
      },
      "source": [
        "## 3.Preprocessing the data\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFMkFPgY1Tov"
      },
      "source": [
        "###### Dropping the useless columns\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtG1EuIB0sZ1",
        "outputId": "eeb8d24d-e097-4694-f836-5c64c123198b"
      },
      "source": [
        "train_data.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['sentiment', 'id', 'date', 'query', 'user', 'text'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QieM_H560w9Y"
      },
      "source": [
        "nr_cols = [\"id\" , \"date\" , \"query\" , \"user\"]\n",
        "\n",
        "train_data1 = train_data.drop(nr_cols, axis = 1)\n",
        "test_data1 = test_data.drop(nr_cols, axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T6XrLci1czY"
      },
      "source": [
        "###### Using re and beautiful soup to clean my tweet_texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wD5d_gZIvTq3"
      },
      "source": [
        "def clean_tweet(tweet):\n",
        "  '''\n",
        "  Input: An uncleaned tweet of \"lxml\" format containg Twitter features like @user_name, https:// etc\n",
        "\n",
        "  Output: A cleaned output\n",
        "  '''\n",
        "\n",
        "  #converting the format to text\n",
        "  tweet = BeautifulSoup(tweet, \"lxml\").get_text()\n",
        "\n",
        "  #using regular expressions\n",
        "  tweet = re.sub(r\"@[A-Za-z0-9]+\" , \" \", tweet)    #remove usernames\n",
        "  tweet = re.sub(r\"https?://[A-Za-z0-9./]+\", \" \", tweet)  #remove hyperlinks\n",
        "  tweet = re.sub(r\"[^A-Za-z0-0.?!]\", \" \", tweet)     #any other non-significant character\n",
        "  tweet = re.sub(r\" +\", \" \", tweet)    #multiple continuous instances of white_space\n",
        "\n",
        "  return tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcjmxbAa4YdW"
      },
      "source": [
        "data = train_data1\n",
        "data_t = test_data1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_eVI3KW5pk2"
      },
      "source": [
        "data_clean = [clean_tweet(x) for x in data.text]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vw369d8V5ppw"
      },
      "source": [
        "data_labels = data.sentiment.values #converting panda series to numpy array\n",
        "data_labels[data_labels == 4] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIP14XZh6nPj"
      },
      "source": [
        "###### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krh_NPRd5psR",
        "outputId": "a345a4e3-cf8e-490b-aed4-5b0ac80a5829"
      },
      "source": [
        "for i in range(10):\n",
        "  print(\"raw data at index\" + str(i) + \":  \" + data.text[i])\n",
        "  print(\"cleaned data at index\" + str(i) + \":  \" + data_clean[i])\n",
        "  print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "raw data at index0:  @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\n",
            "cleaned data at index0:   Awww that s a bummer. You shoulda got David Carr of Third Day to do it. D\n",
            "\n",
            "\n",
            "raw data at index1:  is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\n",
            "cleaned data at index1:  is upset that he can t update his Facebook by texting it... and might cry as a result School today also. Blah!\n",
            "\n",
            "\n",
            "raw data at index2:  @Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds\n",
            "cleaned data at index2:   I dived many times for the ball. Managed to save 0 The rest go out of bounds\n",
            "\n",
            "\n",
            "raw data at index3:  my whole body feels itchy and like its on fire \n",
            "cleaned data at index3:  my whole body feels itchy and like its on fire \n",
            "\n",
            "\n",
            "raw data at index4:  @nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there. \n",
            "cleaned data at index4:   no it s not behaving at all. i m mad. why am i here? because I can t see you all over there. \n",
            "\n",
            "\n",
            "raw data at index5:  @Kwesidei not the whole crew \n",
            "cleaned data at index5:   not the whole crew \n",
            "\n",
            "\n",
            "raw data at index6:  Need a hug \n",
            "cleaned data at index6:  Need a hug \n",
            "\n",
            "\n",
            "raw data at index7:  @LOLTrish hey  long time no see! Yes.. Rains a bit ,only a bit  LOL , I'm fine thanks , how's you ?\n",
            "cleaned data at index7:   hey long time no see! Yes.. Rains a bit only a bit LOL I m fine thanks how s you ?\n",
            "\n",
            "\n",
            "raw data at index8:  @Tatiana_K nope they didn't have it \n",
            "cleaned data at index8:   K nope they didn t have it \n",
            "\n",
            "\n",
            "raw data at index9:  @twittera que me muera ? \n",
            "cleaned data at index9:   que me muera ? \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwIRTLhZ6p-s"
      },
      "source": [
        "vocab_size = 2**16\n",
        "\n",
        "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    data_clean, target_vocab_size = vocab_size)\n",
        "\n",
        "data_inputs = [tokenizer.encode(sentence) for sentence in data_clean]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDDx-mWxQbU8"
      },
      "source": [
        "###### Padding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hb-wk0s0Q2Kq",
        "outputId": "71f431ed-523f-4aa9-d2ee-2d6e5cc47272"
      },
      "source": [
        "print(data_inputs[0])\n",
        "print(\"\\n\")\n",
        "print(data_inputs[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[65168, 1550, 20, 13, 6, 3546, 1, 116, 5218, 50, 1406, 34706, 17, 13224, 593, 3, 49, 79, 1, 65204]\n",
            "\n",
            "\n",
            "[12, 1077, 20, 96, 34, 16, 743, 194, 1807, 124, 2944, 79, 27, 9, 325, 800, 77, 6, 3614, 1736, 76, 3006, 1, 6358, 65169]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XNmayvH6qBH",
        "outputId": "2bb06d16-c2a7-4a7c-b05b-564e830874de"
      },
      "source": [
        "for i in range(10):\n",
        "  print(\"Length of the input \" + str(i) + \" : \" + str(len(data_inputs[i])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of the input 0 : 20\n",
            "Length of the input 1 : 25\n",
            "Length of the input 2 : 21\n",
            "Length of the input 3 : 10\n",
            "Length of the input 4 : 28\n",
            "Length of the input 5 : 5\n",
            "Length of the input 6 : 3\n",
            "Length of the input 7 : 25\n",
            "Length of the input 8 : 8\n",
            "Length of the input 9 : 6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kxo6es4XK3rX"
      },
      "source": [
        "#Thus every input has a different size and we need to make them of the same size. Thus, we need to add some kind of padding to make them of uniform shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ryev8UTK3u5"
      },
      "source": [
        "#finding the len of maximum input\n",
        "MAX_LEN = max([len(x) for x in data_inputs])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXFoo70yK3yj"
      },
      "source": [
        "#padding the data points of data_clean with 0, at the end of the sequence with max_len = MAX_LEN\n",
        "data_inputs = tf.keras.preprocessing.sequence.pad_sequences(data_inputs,\n",
        "                                                            value=0,\n",
        "                                                            padding=\"post\",\n",
        "                                                            maxlen=MAX_LEN)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRznD7iY6qEl",
        "outputId": "96fc4dc8-e5a5-4562-9a0d-60265329700b"
      },
      "source": [
        "#post padding\n",
        "for i in range(100):\n",
        "  print(\"Length of the input \" + str(i) + \" : \" + str(len(data_inputs[i])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of the input 0 : 74\n",
            "Length of the input 1 : 74\n",
            "Length of the input 2 : 74\n",
            "Length of the input 3 : 74\n",
            "Length of the input 4 : 74\n",
            "Length of the input 5 : 74\n",
            "Length of the input 6 : 74\n",
            "Length of the input 7 : 74\n",
            "Length of the input 8 : 74\n",
            "Length of the input 9 : 74\n",
            "Length of the input 10 : 74\n",
            "Length of the input 11 : 74\n",
            "Length of the input 12 : 74\n",
            "Length of the input 13 : 74\n",
            "Length of the input 14 : 74\n",
            "Length of the input 15 : 74\n",
            "Length of the input 16 : 74\n",
            "Length of the input 17 : 74\n",
            "Length of the input 18 : 74\n",
            "Length of the input 19 : 74\n",
            "Length of the input 20 : 74\n",
            "Length of the input 21 : 74\n",
            "Length of the input 22 : 74\n",
            "Length of the input 23 : 74\n",
            "Length of the input 24 : 74\n",
            "Length of the input 25 : 74\n",
            "Length of the input 26 : 74\n",
            "Length of the input 27 : 74\n",
            "Length of the input 28 : 74\n",
            "Length of the input 29 : 74\n",
            "Length of the input 30 : 74\n",
            "Length of the input 31 : 74\n",
            "Length of the input 32 : 74\n",
            "Length of the input 33 : 74\n",
            "Length of the input 34 : 74\n",
            "Length of the input 35 : 74\n",
            "Length of the input 36 : 74\n",
            "Length of the input 37 : 74\n",
            "Length of the input 38 : 74\n",
            "Length of the input 39 : 74\n",
            "Length of the input 40 : 74\n",
            "Length of the input 41 : 74\n",
            "Length of the input 42 : 74\n",
            "Length of the input 43 : 74\n",
            "Length of the input 44 : 74\n",
            "Length of the input 45 : 74\n",
            "Length of the input 46 : 74\n",
            "Length of the input 47 : 74\n",
            "Length of the input 48 : 74\n",
            "Length of the input 49 : 74\n",
            "Length of the input 50 : 74\n",
            "Length of the input 51 : 74\n",
            "Length of the input 52 : 74\n",
            "Length of the input 53 : 74\n",
            "Length of the input 54 : 74\n",
            "Length of the input 55 : 74\n",
            "Length of the input 56 : 74\n",
            "Length of the input 57 : 74\n",
            "Length of the input 58 : 74\n",
            "Length of the input 59 : 74\n",
            "Length of the input 60 : 74\n",
            "Length of the input 61 : 74\n",
            "Length of the input 62 : 74\n",
            "Length of the input 63 : 74\n",
            "Length of the input 64 : 74\n",
            "Length of the input 65 : 74\n",
            "Length of the input 66 : 74\n",
            "Length of the input 67 : 74\n",
            "Length of the input 68 : 74\n",
            "Length of the input 69 : 74\n",
            "Length of the input 70 : 74\n",
            "Length of the input 71 : 74\n",
            "Length of the input 72 : 74\n",
            "Length of the input 73 : 74\n",
            "Length of the input 74 : 74\n",
            "Length of the input 75 : 74\n",
            "Length of the input 76 : 74\n",
            "Length of the input 77 : 74\n",
            "Length of the input 78 : 74\n",
            "Length of the input 79 : 74\n",
            "Length of the input 80 : 74\n",
            "Length of the input 81 : 74\n",
            "Length of the input 82 : 74\n",
            "Length of the input 83 : 74\n",
            "Length of the input 84 : 74\n",
            "Length of the input 85 : 74\n",
            "Length of the input 86 : 74\n",
            "Length of the input 87 : 74\n",
            "Length of the input 88 : 74\n",
            "Length of the input 89 : 74\n",
            "Length of the input 90 : 74\n",
            "Length of the input 91 : 74\n",
            "Length of the input 92 : 74\n",
            "Length of the input 93 : 74\n",
            "Length of the input 94 : 74\n",
            "Length of the input 95 : 74\n",
            "Length of the input 96 : 74\n",
            "Length of the input 97 : 74\n",
            "Length of the input 98 : 74\n",
            "Length of the input 99 : 74\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVvbn9Y4UIeZ"
      },
      "source": [
        "##### Splitting into Training/Test dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5JOS0J86qIS"
      },
      "source": [
        "#The data is in sorted manner. First 800000 datapoints with negative labels, and next 800000 with positive labels\n",
        "\n",
        "test_idx = np.random.randint(0,800000, 8000)   #generate random int as index location between [0, 800000)\n",
        "test_idx = np.concatenate((test_idx, test_idx +800000), axis = 0)    #add 800000 to each input in test_idx to get index location of positive labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F25g5rDRWo2W",
        "outputId": "e6b25934-7c84-409f-cbe2-c04e64da662a"
      },
      "source": [
        "type(data_inputs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sh-o0ZLkVWv4"
      },
      "source": [
        "#Getting test data\n",
        "\n",
        "test_inputs = data_inputs[test_idx]     #index location given by test_idx to data_labels array\n",
        "test_labels = data_labels[test_idx]     #index location given by test_idx to data_labels array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvXRcT5nVWx_"
      },
      "source": [
        "#Getting train_data\n",
        "\n",
        "train_inputs = np.delete(data_inputs, test_idx, axis=0)   ##np.delete deletes the data at the index location given here\n",
        "train_labels = np.delete(data_labels, test_idx)           #np.delete deletes the data at the index location given here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lP285Q-Ytrn"
      },
      "source": [
        "## 4. Model Building"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-FlKPECYyGs"
      },
      "source": [
        "#Building a custom model\n",
        "\n",
        "\n",
        "class DCNN(tf.keras.Model):\n",
        "\n",
        "\n",
        "#voacb_size : to be given by user\n",
        "#emb_dim :  dimension of the embeddings\n",
        "#nb_filters : no. of instances of filters of each type\n",
        "#FFN_units = no. of fully connected neurons in the FFN \n",
        "#nb_classes = no. of classes\n",
        "#dropout_rate: For FFN for regularization\n",
        "#training : (True/False) to indicate the status of model i.e. training or inference\n",
        "#name  = name given \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def __init__(self,\n",
        "                vocab_size,\n",
        "                emb_dim=128,\n",
        "                nb_filters=50,\n",
        "                FFN_units=512,\n",
        "                nb_classes=2,\n",
        "                dropout_rate=0.1,\n",
        "                training=False,\n",
        "                name=\"dcnn\"):\n",
        "    \n",
        "    super(DCNN, self).__init__(name = name)\n",
        "    #defining the various parts of the model\n",
        "\n",
        "    self.embeddings = layers.Embedding(vocab_size, \n",
        "                                       emb_dim)\n",
        "    self.bigram = layers.Conv1D(filters = nb_filters,\n",
        "                                kernel_size = 2,\n",
        "                                padding = \"valid\",\n",
        "                                activation = \"relu\")\n",
        "    self.trigram = layers.Conv1D(filters = nb_filters,\n",
        "                                kernel_size = 3,\n",
        "                                padding = \"valid\",\n",
        "                                activation = \"relu\")\n",
        "    self.fourgram = layers.Conv1D(filters = nb_filters,\n",
        "                                kernel_size = 4,\n",
        "                                padding = \"valid\",\n",
        "                                activation = \"relu\")\n",
        "    \n",
        "    self.pool = layers.GlobalMaxPooling1D()\n",
        "\n",
        "    self.dense_1 = layers.Dense(units = FFN_units, activation = \"relu\")\n",
        "    self.dropout = layers.Dropout(rate = dropout_rate)\n",
        "\n",
        "    #no. of classes basis the classification task\n",
        "    if nb_classes == 2:\n",
        "      self.last_dense = layers.Dense(units=1,\n",
        "                                    activation=\"sigmoid\")\n",
        "    else:\n",
        "      self.last_dense = layers.Dense(units=nb_classes,\n",
        "                                    activation=\"softmax\")\n",
        "\n",
        "\n",
        "    #definition fo my custom call\n",
        "\n",
        "  def call(self, inputs, training):    #trainng is a boolean here\n",
        "    x = self.embeddings(inputs)\n",
        "    x_1 = self.bigram(x)\n",
        "    x_1 = self.pool(x_1)\n",
        "    x_2 = self.trigram(x)\n",
        "    x_2 = self.pool(x_2)\n",
        "    x_3 = self.fourgram(x)\n",
        "    x_3 = self.pool(x_3)\n",
        "\n",
        "    #merging of the 3 outputs\n",
        "    merged = tf.concat([x_1,x_2,x_3], axis = -1)      #Data format: (batch_size, 3 * nb_filters) thus, -1 ensures along the last axis\n",
        "    merged = self.dense_1(merged)\n",
        "    merged = self.dropout(merged, training)\n",
        "    output = self.last_dense(merged)\n",
        "\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xsv8xuCC6U0Y"
      },
      "source": [
        "###### Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e43BcETtYyIe"
      },
      "source": [
        "VOCAB_SIZE = tokenizer.vocab_size\n",
        "\n",
        "EMB_DIM = 200\n",
        "NB_FILTERS = 100\n",
        "FFN_UNITS = 256\n",
        "NB_CLASSES = 2 #len(set(train_labels))\n",
        "\n",
        "DROPOUT_RATE = 0.2\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "NB_EPOCHS = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUupKfrP88Qw"
      },
      "source": [
        "#### Training Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSv344EL872M"
      },
      "source": [
        "Dcnn = DCNN(vocab_size=VOCAB_SIZE,\n",
        "            emb_dim=EMB_DIM,\n",
        "            nb_filters=NB_FILTERS,\n",
        "            FFN_units=FFN_UNITS,\n",
        "            nb_classes=NB_CLASSES,\n",
        "            dropout_rate=DROPOUT_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9ZpGIvOYyN2"
      },
      "source": [
        "if NB_CLASSES == 2:\n",
        "    Dcnn.compile(loss=\"binary_crossentropy\",\n",
        "                 optimizer=\"adam\",\n",
        "                 metrics=[\"accuracy\"])\n",
        "else:\n",
        "    Dcnn.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                 optimizer=\"adam\",\n",
        "                 metrics=[\"sparse_categorical_accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSYY0BnsYyMa"
      },
      "source": [
        "checkpoint_path = \"./content/drive/MyDrive/NLP_CNN/\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(Dcnn=Dcnn)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Latest checkpoint restored!!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6u7XfjZcVW0N"
      },
      "source": [
        "Dcnn.fit(train_inputs,\n",
        "         train_labels,\n",
        "         batch_size=BATCH_SIZE,\n",
        "         epochs = NB_EPOCHS)\n",
        "ckpt_manager.save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jt2dRZWhKHbT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b006cd5-77a3-4f3c-cae1-d4079635e815"
      },
      "source": [
        "results = Dcnn.evaluate(test_inputs, test_labels, batch_size=BATCH_SIZE)\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 11s 23ms/step - loss: 0.4494 - accuracy: 0.7833\n",
            "[0.4493756592273712, 0.7833124995231628]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "_dblHsuABHF7",
        "outputId": "abfc9a47-a057-4b92-b444-a3fc35eae7d0"
      },
      "source": [
        "Dcnn(np.array([tokenizer.encode(\"bad teacher\")]), training=False).numpy()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-159-c874d5d06334>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mDcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bad teacher\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-152-53a10c9fde91>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mx_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrigram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mx_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mx_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfourgram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0mx_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    245\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_causal_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convolution_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconvolution_v2\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1016\u001b[0m       \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1018\u001b[0;31m       name=name)\n\u001b[0m\u001b[1;32m   1019\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconvolution_internal\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name, call_from_convolution, num_spatial_dims)\u001b[0m\n\u001b[1;32m   1146\u001b[0m           \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m           \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m           name=name)\n\u001b[0m\u001b[1;32m   1149\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mchannel_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m                   \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'in a future version'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m                   if date is None else ('after %s' % date), instructions)\n\u001b[0;32m--> 574\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     doc = _add_deprecated_arg_value_notice_to_docstring(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m                   \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'in a future version'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m                   if date is None else ('after %s' % date), instructions)\n\u001b[0;32m--> 574\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     doc = _add_deprecated_arg_value_notice_to_docstring(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconv1d\u001b[0;34m(value, filters, stride, padding, use_cudnn_on_gpu, data_format, name, input, dilations)\u001b[0m\n\u001b[1;32m   1887\u001b[0m           \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1888\u001b[0m           \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1889\u001b[0;31m           name=name)\n\u001b[0m\u001b[1;32m   1890\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1891\u001b[0m       result = squeeze_batch_dims(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m    936\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 938\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    939\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6841\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6842\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6843\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6844\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Computed output size would be negative: -1 [input_size: 2, effective_filter_size: 4, stride: 1] [Op:Conv2D]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBnZXPs2BJ1P",
        "outputId": "c9c6f746-065c-421b-98b7-36cb58510833"
      },
      "source": [
        "tokenizer.encode(\"bad\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[620]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjcYyc50CqyE",
        "outputId": "cf822c7a-e0e9-4a60-e21a-e1615781a930"
      },
      "source": [
        "tokenizer.encode(\"bad teacher\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[132, 8475]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuz3SjaHBJzG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}